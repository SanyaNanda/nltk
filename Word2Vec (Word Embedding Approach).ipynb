{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec is a Word Embedding Approach \n",
    "#### It first converts the words to vectors and then places the similar words or closely used words in a text together, by placing them close as vectors by assigning affinity to the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec](https://user-images.githubusercontent.com/51756349/85860609-b1afd480-b7dc-11ea-9e7e-0aba671fc3d5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_hELlVp9hmZbDZVFstS61pg](https://user-images.githubusercontent.com/51756349/85862210-14a26b00-b7df-11ea-92a2-1aeb2c6b813f.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a paragraph on neural networks\n",
    "para = \"\"\"A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data \n",
    "through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, \n",
    "either organic or artificial in nature. Neural networks can adapt to changing input; so the network generates the best possible \n",
    "result without needing to redesign the output criteria. The concept of neural networks, which has its roots in artificial \n",
    "intelligence, is swiftly gaining popularity in the development of trading systems.Neural networks, in the world of finance, \n",
    "assist in the development of such process as time-series forecasting, algorithmic trading, securities classification, credit \n",
    "risk modeling and constructing proprietary indicators and price derivatives.\n",
    "A neural network works similarly to the human brain’s neural network. A “neuron” in a neural network is a mathematical function \n",
    "that collects and classifies information according to a specific architecture. The network bears a strong resemblance to \n",
    "statistical methods such as curve fitting and regression analysis.\n",
    "A neural network contains layers of interconnected nodes. Each node is a perceptron and is similar to a multiple linear \n",
    "regression. The perceptron feeds the signal produced by a multiple linear regression into an activation function that may be \n",
    "nonlinear.In a multi-layered perceptron (MLP), perceptrons are arranged in interconnected layers. The input layer collects \n",
    "input patterns. The output layer has classifications or output signals to which input patterns may map. For instance, the \n",
    "patterns may comprise a list of quantities for technical indicators about a security; potential outputs could be “buy,” “hold” \n",
    "or “sell.”\n",
    "Hidden layers fine-tune the input weightings until the neural network’s margin of error is minimal. It is hypothesized that \n",
    "hidden layers extrapolate salient features in the input data that have predictive power regarding the outputs. This describes \n",
    "feature extraction, which accomplishes a utility similar to statistical techniques such as principal component analysis.Neural \n",
    "networks are broadly used, with applications for financial operations, enterprise planning, trading, business analytics and \n",
    "product maintenance. Neural networks have also gained widespread adoption in business applications such as forecasting and \n",
    "marketing research solutions, fraud detection and risk assessment.\n",
    "A neural network evaluates price data and unearths opportunities for making trade decisions based on the data analysis. The \n",
    "networks can distinguish subtle nonlinear interdependencies and patterns other methods of technical analysis cannot. According \n",
    "to research, the accuracy of neural networks in making price predictions for stocks differs. Some models predict the correct \n",
    "stock prices 50 to 60 percent of the time while others are accurate in 70 percent of all instances. Some have posited that a 10 \n",
    "percent improvement in efficiency is all an investor can ask for from a neural network.\n",
    "There will always be data sets and task classes that a better analyzed by using previously developed algorithms. It is not so \n",
    "much the algorithm that matters; it is the well-prepared input data on the targeted indicator that ultimately determines the \n",
    "level of success of a neural network.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',para) \n",
    "text = re.sub(r'\\s+',' ',text) \n",
    "text = text.lower() \n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['neural', 'network', 'series', 'algorithms', 'endeavors', 'recognize', 'underlying', 'relationships', 'set', 'data', 'process', 'mimics', 'way', 'human', 'brain', 'operates', '.'], ['sense', ',', 'neural', 'networks', 'refer', 'systems', 'neurons', ',', 'either', 'organic', 'artificial', 'nature', '.'], ['neural', 'networks', 'adapt', 'changing', 'input', ';', 'network', 'generates', 'best', 'possible', 'result', 'without', 'needing', 'redesign', 'output', 'criteria', '.'], ['concept', 'neural', 'networks', ',', 'roots', 'artificial', 'intelligence', ',', 'swiftly', 'gaining', 'popularity', 'development', 'trading', 'systems.neural', 'networks', ',', 'world', 'finance', ',', 'assist', 'development', 'process', 'time-series', 'forecasting', ',', 'algorithmic', 'trading', ',', 'securities', 'classification', ',', 'credit', 'risk', 'modeling', 'constructing', 'proprietary', 'indicators', 'price', 'derivatives', '.'], ['neural', 'network', 'works', 'similarly', 'human', 'brain', '’', 'neural', 'network', '.'], ['“', 'neuron', '”', 'neural', 'network', 'mathematical', 'function', 'collects', 'classifies', 'information', 'according', 'specific', 'architecture', '.'], ['network', 'bears', 'strong', 'resemblance', 'statistical', 'methods', 'curve', 'fitting', 'regression', 'analysis', '.'], ['neural', 'network', 'contains', 'layers', 'interconnected', 'nodes', '.'], ['node', 'perceptron', 'similar', 'multiple', 'linear', 'regression', '.'], ['perceptron', 'feeds', 'signal', 'produced', 'multiple', 'linear', 'regression', 'activation', 'function', 'may', 'nonlinear.in', 'multi-layered', 'perceptron', '(', 'mlp', ')', ',', 'perceptrons', 'arranged', 'interconnected', 'layers', '.'], ['input', 'layer', 'collects', 'input', 'patterns', '.'], ['output', 'layer', 'classifications', 'output', 'signals', 'input', 'patterns', 'may', 'map', '.'], ['instance', ',', 'patterns', 'may', 'comprise', 'list', 'quantities', 'technical', 'indicators', 'security', ';', 'potential', 'outputs', 'could', '“', 'buy', ',', '”', '“', 'hold', '”', '“', 'sell.', '”', 'hidden', 'layers', 'fine-tune', 'input', 'weightings', 'neural', 'network', '’', 'margin', 'error', 'minimal', '.'], ['hypothesized', 'hidden', 'layers', 'extrapolate', 'salient', 'features', 'input', 'data', 'predictive', 'power', 'regarding', 'outputs', '.'], ['describes', 'feature', 'extraction', ',', 'accomplishes', 'utility', 'similar', 'statistical', 'techniques', 'principal', 'component', 'analysis.neural', 'networks', 'broadly', 'used', ',', 'applications', 'financial', 'operations', ',', 'enterprise', 'planning', ',', 'trading', ',', 'business', 'analytics', 'product', 'maintenance', '.'], ['neural', 'networks', 'also', 'gained', 'widespread', 'adoption', 'business', 'applications', 'forecasting', 'marketing', 'research', 'solutions', ',', 'fraud', 'detection', 'risk', 'assessment', '.'], ['neural', 'network', 'evaluates', 'price', 'data', 'unearths', 'opportunities', 'making', 'trade', 'decisions', 'based', 'data', 'analysis', '.'], ['networks', 'distinguish', 'subtle', 'nonlinear', 'interdependencies', 'patterns', 'methods', 'technical', 'analysis', '.'], ['according', 'research', ',', 'accuracy', 'neural', 'networks', 'making', 'price', 'predictions', 'stocks', 'differs', '.'], ['models', 'predict', 'correct', 'stock', 'prices', 'percent', 'time', 'others', 'accurate', 'percent', 'instances', '.'], ['posited', 'percent', 'improvement', 'efficiency', 'investor', 'ask', 'neural', 'network', '.'], ['always', 'data', 'sets', 'task', 'classes', 'better', 'analyzed', 'using', 'previously', 'developed', 'algorithms', '.'], ['much', 'algorithm', 'matters', ';', 'well-prepared', 'input', 'data', 'targeted', 'indicator', 'ultimately', 'determines', 'level', 'success', 'neural', 'network', '.']]\n"
     ]
    }
   ],
   "source": [
    "# words are tokenised and stopwords removed\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "print(sentences) # List of meaningful words in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=2) \n",
    "# min count = 2 means that only those tokens will be considered which have appeared 2 or more times in the \n",
    "# paragraph over all the sentences\n",
    "words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neural': <gensim.models.keyedvectors.Vocab at 0x293d3666fd0>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x293d2dfaa20>,\n",
       " 'algorithms': <gensim.models.keyedvectors.Vocab at 0x293d47749e8>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x293d47749b0>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x293d4774a20>,\n",
       " 'human': <gensim.models.keyedvectors.Vocab at 0x293d4774a58>,\n",
       " 'brain': <gensim.models.keyedvectors.Vocab at 0x293d4774ac8>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x293d4774b00>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x293d4774b70>,\n",
       " 'networks': <gensim.models.keyedvectors.Vocab at 0x293d4774ba8>,\n",
       " 'artificial': <gensim.models.keyedvectors.Vocab at 0x293d4774be0>,\n",
       " 'input': <gensim.models.keyedvectors.Vocab at 0x293d4774c88>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x293d4774cc0>,\n",
       " 'output': <gensim.models.keyedvectors.Vocab at 0x293d4774cf8>,\n",
       " 'development': <gensim.models.keyedvectors.Vocab at 0x293d4774d30>,\n",
       " 'trading': <gensim.models.keyedvectors.Vocab at 0x293d4774da0>,\n",
       " 'forecasting': <gensim.models.keyedvectors.Vocab at 0x293d4774e10>,\n",
       " 'risk': <gensim.models.keyedvectors.Vocab at 0x293d4774828>,\n",
       " 'indicators': <gensim.models.keyedvectors.Vocab at 0x293d4774438>,\n",
       " 'price': <gensim.models.keyedvectors.Vocab at 0x293d4774710>,\n",
       " '’': <gensim.models.keyedvectors.Vocab at 0x293d47744a8>,\n",
       " '“': <gensim.models.keyedvectors.Vocab at 0x293d4774630>,\n",
       " '”': <gensim.models.keyedvectors.Vocab at 0x293d4774940>,\n",
       " 'function': <gensim.models.keyedvectors.Vocab at 0x293d4774518>,\n",
       " 'collects': <gensim.models.keyedvectors.Vocab at 0x293d47745f8>,\n",
       " 'according': <gensim.models.keyedvectors.Vocab at 0x293d47740f0>,\n",
       " 'statistical': <gensim.models.keyedvectors.Vocab at 0x293d4774588>,\n",
       " 'methods': <gensim.models.keyedvectors.Vocab at 0x293d4774160>,\n",
       " 'regression': <gensim.models.keyedvectors.Vocab at 0x293d4774198>,\n",
       " 'analysis': <gensim.models.keyedvectors.Vocab at 0x293d4774208>,\n",
       " 'layers': <gensim.models.keyedvectors.Vocab at 0x293d4774320>,\n",
       " 'interconnected': <gensim.models.keyedvectors.Vocab at 0x293d47742b0>,\n",
       " 'perceptron': <gensim.models.keyedvectors.Vocab at 0x293d4774358>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x293d4774278>,\n",
       " 'multiple': <gensim.models.keyedvectors.Vocab at 0x293d4774390>,\n",
       " 'linear': <gensim.models.keyedvectors.Vocab at 0x293d4774550>,\n",
       " 'may': <gensim.models.keyedvectors.Vocab at 0x293d47744e0>,\n",
       " 'layer': <gensim.models.keyedvectors.Vocab at 0x293d47746d8>,\n",
       " 'patterns': <gensim.models.keyedvectors.Vocab at 0x293d47743c8>,\n",
       " 'technical': <gensim.models.keyedvectors.Vocab at 0x293d4774780>,\n",
       " 'outputs': <gensim.models.keyedvectors.Vocab at 0x293d47748d0>,\n",
       " 'hidden': <gensim.models.keyedvectors.Vocab at 0x293d4777ef0>,\n",
       " 'applications': <gensim.models.keyedvectors.Vocab at 0x293d4777f60>,\n",
       " 'business': <gensim.models.keyedvectors.Vocab at 0x293d4777f28>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x293d4777780>,\n",
       " 'making': <gensim.models.keyedvectors.Vocab at 0x293d4777a58>,\n",
       " 'percent': <gensim.models.keyedvectors.Vocab at 0x293d4777f98>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.2707290e-03 -4.2620939e-03 -9.7396289e-04  2.4291351e-03\n",
      " -1.3892971e-03  4.5290855e-03 -4.2491262e-03 -1.4231686e-03\n",
      " -4.5711634e-04  1.4904573e-03 -4.5003693e-04  3.8633293e-03\n",
      "  4.5335721e-03  3.7968282e-03 -4.5820465e-03 -2.9537515e-03\n",
      "  3.2541754e-03 -1.6681055e-03 -2.9305071e-03  2.4236403e-03\n",
      " -3.0991794e-03  3.7775543e-03  2.9707695e-03  3.7834714e-03\n",
      "  1.3560386e-03  3.6193265e-03 -4.9997629e-03  4.5577675e-04\n",
      "  4.5338240e-03  2.6881774e-03 -3.2558830e-03 -4.6232748e-03\n",
      " -3.1811534e-04 -3.9419634e-03  2.7059070e-03  3.5311822e-03\n",
      "  3.7677403e-04 -3.7658469e-05 -3.4251995e-03 -8.1352255e-04\n",
      " -2.5429304e-03  9.4373716e-04 -3.5800645e-03  4.5270859e-03\n",
      " -3.9450689e-03  6.9655693e-04  1.5461041e-03  1.7647581e-03\n",
      " -3.4081824e-03  8.3387182e-05  4.4232449e-03 -4.1057039e-03\n",
      " -1.9614575e-03  2.4817986e-03 -3.5980509e-03  4.3026106e-03\n",
      " -1.9736392e-03 -3.9448808e-03  2.2245373e-03  2.6434494e-04\n",
      " -1.7638891e-03  3.1780011e-03  8.7420369e-04  4.2032823e-03\n",
      " -3.6597179e-05 -7.5495459e-06  1.9148826e-03  4.1482663e-03\n",
      " -2.9623166e-03 -3.0458795e-03  3.8945712e-03  2.5826234e-03\n",
      "  2.7269456e-03 -3.7233231e-03 -3.5987268e-03  3.6094333e-03\n",
      " -1.5597350e-03 -4.6102380e-04 -2.4518170e-03  1.0598364e-03\n",
      "  4.8816316e-03 -9.8127901e-05  4.5404532e-03  1.7110587e-03\n",
      " -2.5000386e-03 -3.0415433e-03 -2.8958628e-03  3.8286201e-03\n",
      "  4.0390347e-03  1.7731552e-03  1.2552127e-04 -5.6312745e-04\n",
      " -7.5357023e-04 -4.8811682e-03  3.6791072e-03 -5.7810603e-04\n",
      "  7.9301023e-04 -1.3293324e-03 -4.1112225e-03  1.8938838e-03]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv['analysis']\n",
    "print(vector)\n",
    "print(len(vector)) # these are points that the word 'analysis' marks over 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hidden', 0.1423044502735138),\n",
       " ('outputs', 0.1080545112490654),\n",
       " ('forecasting', 0.09159283339977264),\n",
       " ('statistical', 0.0738331601023674),\n",
       " ('data', 0.07324998080730438),\n",
       " ('regression', 0.07109078019857407),\n",
       " ('similar', 0.06977398693561554),\n",
       " ('neural', 0.06019268557429314),\n",
       " ('risk', 0.05840431898832321),\n",
       " ('methods', 0.05552846938371658)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates a list of words closely used with word mentioned\n",
    "similar = model.wv.most_similar('artificial')\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interconnected', 0.2929634749889374),\n",
       " ('.', 0.16374357044696808),\n",
       " ('making', 0.16234111785888672),\n",
       " ('similar', 0.15972241759300232),\n",
       " ('network', 0.11722555756568909),\n",
       " ('business', 0.1146617978811264),\n",
       " ('data', 0.11317825317382812),\n",
       " ('layers', 0.10206985473632812),\n",
       " ('may', 0.0993332713842392),\n",
       " ('trading', 0.09606222063302994)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = model.wv.most_similar('neural')\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('function', 0.22197595238685608),\n",
       " ('human', 0.14206382632255554),\n",
       " ('layer', 0.13823407888412476),\n",
       " ('statistical', 0.13101479411125183),\n",
       " ('collects', 0.12973207235336304),\n",
       " (',', 0.12816891074180603),\n",
       " ('perceptron', 0.12561340630054474),\n",
       " ('multiple', 0.12113900482654572),\n",
       " ('outputs', 0.11764165014028549),\n",
       " ('linear', 0.11197951436042786)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = model.wv.most_similar('price')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec has 2 neural network models used to vectorize the words\n",
    "### 1. CBOW - Continuous Bag of words\n",
    "### 2. Skip Gram\n",
    "#### In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. \n",
    "#### While in the Skip-gram model, the distributed representation of the input word is used to predict the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture-of-Word2Vec-models-CBOW-and-Skip-Gram](https://user-images.githubusercontent.com/51756349/85862530-84b0f100-b7df-11ea-997e-b74b261cbc13.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'neural' and 'networks' - CBOW :  0.031239564\n",
      "Cosine similarity between 'neural' and 'statistical' - CBOW :  0.12230882\n",
      "Cosine similarity between 'neural' and 'networks' - Skip Gram :  0.3217716\n",
      "Cosine similarity between 'neural' and 'statistical' - Skip Gram :  0.29052967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\sanya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n",
      "c:\\users\\sanya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\sanya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Create CBOW model \n",
    "model1 = Word2Vec(sentences, min_count = 1,  size = 200, window = 20) \n",
    "print(\"Cosine similarity between 'neural' \" + \"and 'networks' - CBOW : \", model1.similarity('neural', 'networks')) \n",
    "      \n",
    "print(\"Cosine similarity between 'neural' \" +\"and 'statistical' - CBOW : \", model1.similarity('neural', 'statistical')) \n",
    "  \n",
    "# Create Skip Gram model \n",
    "model2 = Word2Vec(sentences, min_count = 1, size = 200, window = 20, sg = 1)  \n",
    "print(\"Cosine similarity between 'neural' \" + \"and 'networks' - Skip Gram : \", model2.similarity('neural', 'networks')) \n",
    "      \n",
    "print(\"Cosine similarity between 'neural' \" +\"and 'statistical' - Skip Gram : \", model2.similarity('neural', 'statistical')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
